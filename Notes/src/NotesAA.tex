\documentclass[11pt,a4paper]{article}

\usepackage[margin=1in, paperwidth=8.3in, paperheight=11.7in]{geometry}
\usepackage[ruled, vlined, linesnumbered]{algorithm2e}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{changepage} 

\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{xcolor}

\lstset{
    frameround=fttt,
    language=Prolog,
    numbers=left,
    breaklines=true,
    keywordstyle=\color{blue}\bfseries, 
    basicstyle=\ttfamily\color{red},
    numberstyle=\color{black}
    }

\begin{document}

\pagestyle{fancy}
\setlength\parindent{0pt}
\allowdisplaybreaks

\renewcommand{\headrulewidth}{0pt}
\setlist[enumerate,1]{label={\roman*)}}

% Cover page title
\title{Advanced Algorithms - Notes}
\author{Dom Hutchinson}
\date{\today}
\maketitle

% Header
\fancyhead[L]{Dom Hutchinson}
\fancyhead[C]{Advanced Algorithms - Notes}
\fancyhead[R]{\today}

% Counters
\newcounter{definition}[section]
\newcounter{example}[section]
\newcounter{notation}[section]
\newcounter{proposition}[section]
\newcounter{proof}[section]
\newcounter{remark}[section]
\newcounter{theorem}[section]

% commands
\newcommand{\dotprod}[0]{\boldsymbol{\cdot}}
\newcommand{\cosech}[0]{\mathrm{cosech}\ }
\newcommand{\cosec}[0]{\mathrm{cosec}\ }
\newcommand{\sech}[0]{\mathrm{sech}\ }
\newcommand{\prob}[0]{\mathbb{P}}
\newcommand{\nats}[0]{\mathbb{N}}
\newcommand{\cov}[0]{\mathrm{Cov}}
\newcommand{\var}[0]{\mathrm{Var}}
\newcommand{\expect}[0]{\mathbb{E}}
\newcommand{\reals}[0]{\mathbb{R}}
\newcommand{\integers}[0]{\mathbb{Z}}
\newcommand{\indicator}[0]{\mathds{1}}
\newcommand{\nb}[0]{\textit{N.B.} }
\newcommand{\ie}[0]{\textit{i.e.} }
\newcommand{\eg}[0]{\textit{e.g.} }
\newcommand{\X}[0]{\textbf{X}}
\newcommand{\x}[0]{\textbf{x}}
\newcommand{\iid}[0]{\overset{\text{iid}}{\sim}}
\newcommand{\proved}[0]{$\hfill\square$\\}
\newcommand{\Mod}[1]{\ \mathrm{mod}\ #1}

\newcommand{\definition}[1]{\stepcounter{definition} \textbf{Definition \arabic{section}.\arabic{definition}\ - }\textit{#1}\\}
\newcommand{\definitionn}[1]{\stepcounter{definition} \textbf{Definition \arabic{section}.\arabic{definition}\ - }\textit{#1}}
\newcommand{\proof}[1]{\stepcounter{proof} \textbf{Proof \arabic{section}.\arabic{proof}\ - }\textit{#1}\\}
\newcommand{\prooff}[1]{\stepcounter{proof} \textbf{Proof \arabic{section}.\arabic{proof}\ - }\textit{#1}}
\newcommand{\example}[1]{\stepcounter{example} \textbf{Example \arabic{section}.\arabic{example}\ - }\textit{#1}\\}
\newcommand{\examplee}[1]{\stepcounter{example} \textbf{Example \arabic{section}.\arabic{example}\ - }\textit{#1}}
\newcommand{\notation}[1]{\stepcounter{notation} \textbf{Notation \arabic{section}.\arabic{notation}\ - }\textit{#1}\\}
\newcommand{\notationn}[1]{\stepcounter{notation} \textbf{Notation \arabic{section}.\arabic{notation}\ - }\textit{#1}}
\newcommand{\proposition}[1]{\stepcounter{proposition} \textbf{Proposition \arabic{section}.\arabic{proposition}\ - }\textit{#1}\\}
\newcommand{\propositionn}[1]{\stepcounter{proposition} \textbf{Proposition \arabic{section}.\arabic{proposition}\ - }\textit{#1}}
\newcommand{\remark}[1]{\stepcounter{remark} \textbf{Remark \arabic{section}.\arabic{remark}\ - }\textit{#1}\\}
\newcommand{\remarkk}[1]{\stepcounter{remark} \textbf{Remark \arabic{section}.\arabic{remark}\ - }\textit{#1}}
\newcommand{\theorem}[1]{\stepcounter{theorem} \textbf{Theorem \arabic{section}.\arabic{theorem}\ - }\textit{#1}\\}
\newcommand{\theoremm}[1]{\stepcounter{theorem} \textbf{Theorem \arabic{section}.\arabic{theorem}\ - }\textit{#1}}

\tableofcontents

% Start of content
\newpage

\section{Hashing}

\definition{Dictionary}
A \textit{Dictionary} is an abstract data structure which stores $(\textit{key},\textit{value})$ pairs, with \textit{key} being unique.\\
A \textit{Dynamic Dictionary} can perform the following operations
\begin{center}
\begin{tabular}{l|l}
\textbf{Operation}&\textbf{Description}\\\hline
\lstinline!add(k,v)!&Add the pair \lstinline!(k,v)!.\\
\lstinline!lookup(k)!&Return \lstinline!v! if \lstinline!(k,v)! is in dictionary, \lstinline!NULL! otherwise.\\
\lstinline!delete(k)!&Remove pair \lstinline!(k,v)!, assuming \lstinline!(k,v)! is in dictionary.
\end{tabular}
\end{center}
A \textit{Static Dictionary} can only perform lookups, after it has been built.
\begin{center}
\begin{tabular}{l|l}
\textbf{Operation}&\textbf{Description}\\\hline
\lstinline!lookup(k)!&Return \lstinline!v! if \lstinline!(k,v)! is in dictionary, \lstinline!NULL! otherwise.
\end{tabular}
\end{center}

\proposition{Implementing a Dictionary}
Many data structures can be used to implement a \textit{Dictionary}.\\
These include, but not limited to:
\begin{enumerate}
	\item Linked lists.
	\item Binary Search, (2,3,4) \& Red-Black Trees.
	\item Skip lists
	\item van Emde Boas Trees.
\end{enumerate}

\remark{Motivation for Hashing}
None of the implementations of a \textit{Dictionary} suggested in \textbf{Proposition 1.1} achieves a $O(1)$ run-time complexity in the worst case for \underline{all} operations. To achieve this we introduce \textit{Hashing}.\\

\definition{Hash Function}
A \textit{Hash Function} takes in object's key and returns a value which is used to index the object in a \textit{Hash Table}.\\
Let $S$ be the set of all possible keys a hash function can recieve \& $m$ be the number of indexes in its \textit{Associated Hash Table}. Then
$$h:S\to[m]$$
\nb We want to avoid cases where $h(x)=h(y)$ for $x\neq y$(\textit{collisions}) .\\

\remark{Hashing functions assign items to indices with a geometric distribution}

\remark{Avoiding Collisions in Hashing}
When indexing $n$ items to $m$ indicies using a \textit{Hash Function} we only avoid \textit{Collisions} if $m\gg n$.\\

\definition{Hash Table}
A \textit{Hash Table} is an abstract data structre which extends the \textit{Dictionary} in such a way that time complexity is reduced.\\
A \textit{Hash Table} is comprise of an array \& a \textit{Hash Function}. The \textit{Hash Function} maps an object's key to an index in the array. If multiple objects have the same \textit{Hash Value} then a \textit{Linked List} is used in that index, with new objects added to the end of the \textit{Linked List} (Called \textit{Chaining}).\\

\proposition{Time Complexity for Dictionary Operations in a Hash Table}
By building a \textit{Hash Table} with \textit{Chaining} we achieve the following time complexities for \textit{Dictionary} operations
\begin{center}
\begin{tabular}{l|l|l}
\textbf{Operation}&\textbf{Worst Case Time Complexity}&Comments\\\hline
\lstinline!add(k,v)!&$O(1)$&Add item to the end of \textit{Linked List} if necessary.\\
\lstinline!lookup(k)!&$O($length of chain containing \lstinline!k!$)$&We might have to search through the whole\\&&\textit{Linked List} containing \lstinline!k!.\\
\lstinline!delete(k)!&$O($length of chain containing \lstinline!k!$)$&Only $O(1)$ to perform the actual deletion,\\
&&but need to find \lstinline!k! first.
\end{tabular}
\end{center}

\theorem{True Randomness}
Consider $n$ fixed inputs for a \textit{Hash Table} with $m$ indices. (\ie any sequence of $n$ \lstinline!add!/\lstinline!lookup!/\lstinline!delete! operations).\\
Pick a \textit{Hash Function}, $h$, at random from a set of all \textit{Hash Functions}, $H:=\{h:S\to[m])$. Then
$$\expect(\text{Run-Time per Operation})=O\left(1+\frac{n}{m}\right)$$
\nb The expected run-time per operation is $O(1)$ if $m\gg n$.\\

\proof{Theorem 1.1}
Let $x\ \&\ y\in S$ be two distincy keys \& $T$ be a \textit{Hash Table} with $m$ indexes.\\
Define $I_{x,y}\begin{cases}1&h(x)=h(y)\\0&\text{otherwise}\end{cases}$.\\
We have $\prob(h(x)=h(y))=\frac{1}{m}$.\\
Therefore
\[\begin{array}{rcl}
\expect(I_{x,y})&=&\prob(I_{x,y}=1)\\
&=&\prob(h(x)=h(y))\\
&=&\frac{1}{m}
\end{array}\]
Let $N_x$ be the number of keys stored in $H$ that are hashed to $h(x)$.\\
Note that $N_x=\displaystyle\sum_{k\in T}I_{x,k}$.\\
Now we have that
$$\expect(N_x)=\expect\left(\displaystyle\sum_{k\in T}I_{x,k}\right)=\sum_{k\in H}\expect(I_{x,k})=n\frac{1}{m}=\frac{n}{m}$$
\proved

\remark{Why not hash to unique values}
Suppose we want to define a \textit{Hash Function} which maps each key in $S$ to a unique position in the \textit{Hash Table}, $T$. This requires $m$ unique positions, which in turn require $\log_2 m$ bits for each key. This is an unreasonably large amount of space.\\

\proposition{Specifying the Hash Function}
Consider a set of \textit{Hash Functions}, $H:=\{h_1,h_2,\dots\}$.\\
When we initialise a \textit{Hash Table} we choose a hash function $h\in H$ at random and then proceed only to use $h$ when dealing with this specific \textit{Hash Table}.\\

\remark{Randomness in Hashing}
All the randomness in \textit{Hashing} comes from how we choose the \textit{Hash Function} \& not from how the \textit{Hash Function} itself runs.\\

\definition{Weakly Universal Set of Hashing Functions}
Let $H:=\{h|h:S\to[m]\}$ be a set of \textit{Hashing Functions}.\\
$H$ is \textit{Weakly Universal} if for any chosen $x,y\in S$ with $x\neq y$
$$\prob(h(x)=h(y))\leq\frac{1}{m}\text{ when varying }h(\cdot)$$
when $h$ is chosen uniformly at random from $H$.\\

\theorem{Expected Run time for Weakly Universal Set}
Consider $n$ fixeds to a \textit{Hash Table}, $T$, with $m$ indexes.\\
Pick a \textit{Hash Function}, $H$, from a \textit{Weakly Universal Set} of \textit{Hash Functions}, $H$.
$$\expect(\text{Run-Time per Operation})=O(1)\text{ for }m\geq n$$
\nb Proof is same as for \textit{True Randomness}.\\

\proposition{Constructing a Weakly Universal Set of Hash Functions}
Let $S:=[s]$ be the set of possible keys \& $p$ be some prime greater than $s$\footnote{There is a theorem that $\forall\ n\ \exists\ p\in[n,2n]$ st $p$ is prime.}.\\
Choose some $a,b\in[0,p-1]$ \& define
\[\begin{array}{rcl}
h_{a,b}(x)&=&\underbrace{[\ (ax+b)\Mod p\ ]}_\text{spread values over [0,p-1]}\underbrace{\Mod m}_\text{causes collisions}\\
H_{p,m}&=&\{h_{a,b}(\cdot):a\in[1,p-1],\ b\in[0,p-1]
\end{array}\]
\nb $H_{p,m}$ is a \textit{Weakly Universal Set} of \textit{Hashing Functions}.\\
\nb Different values of $a\ \&\ b$ perform differently for different data sets.\\

\remarkk{True Randomness vs Weakly Universal Hashing}
\begin{itemize}
	\item[-] For both \textit{True Randomness} \& \textit{Weakly Universal Hashing} we have that when $m\geq n$ the expected \lstinline!lookup! time in the \textit{Hash Table} is $O(1)$.
	\item[-] Constructing a \textit{Weakly Universal Set} of \textit{Hash Functions} is generally easier.
\end{itemize}

\theorem{Longest Chain - True Randomness}
If \textit{Hashing Function} $h$ is selected uniformly at random from all \textit{Hashing Functions} to $m$ indicies.\\
Then, over $m$ inputs we have
$$\prob(\exists\text{ a chain length}\geq3\log_2 m)\leq\frac1m$$

\proof{Theorem 1.3}
This problem is equivalent to showing that if we randomly throw $m$ balls into $m$ bins the probabiltiy of having a bin with at least $3\log_2 m$ balls is at most $\frac1m$.\\
Let $X_1$ be the number of valls in the first bin.\\
Choose any $k$ of the $M$ balls, the probabiltiy that all of these $K$ balls go into the first bin is $\frac1{m^k}$.\\
By the \textit{Union Bound Theorem} we have
$$\prob(X_1\geq k)\leq{m\choose k}\frac{1}{m^k}\leq1{k!}$$
Applying the \textit{Union Bound Theorem} again we have
$$\prob(\text{at least 1 bin recieves at least }k\text{ balls})\leq m\prob(X_1\geq k)\leq\frac{m}{k!}$$
Observe that
\[\begin{array}{rcl}
k!&>&2^{k-1}\\
\text{Let }k&=&3\log_2 m\\
\implies k!&>&2^{(3\log_2m-1)}\\
&\geq&2^{2\log_2 m}\\
&\geq&(2^{\log_2 m})^2\\
&=&m^2
\end{array}\]
Thus, setting $k=3\log_2 m$ means
$$\frac{m}{k!}\leq\frac1m\text{ for }m\geq2$$
\proved

\theorem{Longest Chain - Weakly Universal Hashing}
Let \textit{Hashing Function} $h$ be picked uniformly at random from a \textit{Weakly Universal Set} of \textit{Hashing Functions}.\\
Then, over $m$ inputs
$$\prob(\exists\text{ a chain length}\geq1+\sqrt{2m})\leq\frac12$$
\nb This is a poor bound.\\

\proof{Theorem 1.4}
Let $x,y\in S$ be two keys and define $I_{x,y}\begin{cases}1&h(x)=h(y)\\0&\text{otherwise}\end{cases}$.\\
Let $C$ be a random variable for the total number of collision (\ie $C=\sum_{x,y\in H, \x<y}I_{x,y}$).\\
Using \textit{Linearity of Expectation} and that $\expect(I_{x,y})=\frac1m$ when $h$ is \textit{Weakly Universal}
$$\expect(C)=\expect\left(\sum_{x,y\in H,\ x<y}I_{x,y}\right)=\sum_{x,y\in H,\ x<y}\expect(I_{x,y})={m\choose2}\frac1m\leq\frac{m}{2}$$
By \textit{Markov's Inequality}
$$\prob(C\geq m)\leq\frac{\expect(C)}m\leq\frac12$$
Let $L$ be a random variable for the length of the longest chain in $H$.\\
Then, $C\leq{L\choose2}$.
Now
$$\prob\left(\frac{(L-1)^2}{2}\geq m\right)\leq\prob\left({L\choose2}\geq m\right)\leq\prob(C\geq m)\leq\frac12$$
By rearranging, we have that
$$\prob(L\geq1+\sqrt{2m})\leq\frac12$$

\subsection{Perfect Hashing}

\remark{Motivation}
The \textit{Hashing Schemes} discussed in the previous part perform well in the best \& average cases but not necessarily in the worst cases (as they can have really long longest chains).\\

\definition{Static Perfect Hashing}
A \textit{Perfect Static Hashing Scheme} is a scheme that produces a \textit{Hash Table} where  \lstinline!lookup! has time complexity $\in O(1)$, even in the worst case. However this \textit{Hash Table} is static so we cannot perform \lstinline!insert! or \lstinline!delete! after the table has been produced.\\
\nb \textit{FKS Hashing Scheme} is a \textit{Perfect Static Hashing Scheme}.\\

\definition{FKS Hashing Scheme}
Below is an algorithm for the \textit{FKS Hashing Scheme}\\
\begin{algorithm}[H]
\SetKwInOut{Require}{require}
\caption{FKS Hashing Scheme}
\Require{$n \{\text{\# insertions}\}, T \{\text{Table with $n$ entries}\}$}
Insert all $n$ into $T$ using $h$\\
\While{Collisions in $T\geq n$}{
	Rebuild $T$ using a new $h$.
}
Let $n_i=|T[i]|$.\\
\For{$i\in[1,n]$}{
	Insert items of $T[i]$ into new table $T_i$ of size $n_i^2$ using $h_i$.\\
	\While{Collisions in $T_i\geq 1$}{
		Rebuild $T_i$ using a new $h_i$.
	}
}
\Return{T}
\end{algorithm}
\nb $\prob(\text{Collisions in }T_i\geq1)\leq\frac12$ and \nb $\prob(\text{Collisions in }T\geq n)\leq\frac12$ so we expect to have to build each table twice.\\

\remark{If $n$ items are mapped to the same index this counts as ${n\choose2}$ collision.}

\proposition{FKS Hashing Scheme - \lstinline!lookup!}
Below is an algorithm for \lstinline!lookup(x)! in the \textit{Hash Tables} produced by the \textit{FKS Hashing Scheme}
\begin{algorithm}[H]
\caption{FKS - \lstinline!lookup(x)!}
\SetKwInOut{Require}{require}
\Require{$T\ \{\text{main table}\}, \{T_1,\dots,T_m\}\ \{\text{sub-tables}\},x\ \{\text{key}\}$}
Compute $i=h(x)$.\\
Compute $j=h_i(x)$.\\
\Return{$T_i[j]$}
\end{algorithm}
\nb This runs in $O(1)$ time.\\

\proof{FKS Hashing Scheme - Space Requirements}
In the \textit{FKS Hashing Scheme} the main table $T$ requires space $O(n)$ and each sub-table $T_i$ requires space $O(n_i^2)$, where $n_i=|T[i]|$.\\
Storing each task function, $h_i$ requires space $O(1)$.\\
Thus the total space used is1
$$O(n)+\sum_iO(n_i^2)=O(n)+O\left(\sum_in_i^2\right)$$
We know there are ${n_i\choose 2}$ collisions in $T[i]$ so there are $\sum_i{n_i\choose 2}$ collisions in $T$.\\
We know there are at most $n$ collisions in $T$ so
$$\sum_i\frac{n_i^2}{4}\leq\sum_i{n_i\choose2}<n\implies\sum_in_i^2<4n$$
Thus
$$O(n)+O\left(\sum_in_i^2\right)=O(n)$$

\proof{FKS Hashing Scheme - Expected Construction Time}
The expected construction time for the main table, $T$, is $O(n)$.\\
The expected construction time for reach sub-table, $T_i$, is $O(n_i^2)$ where $n_i:=|T[i]|$.\\
Thus
\[\begin{array}{rcl}
expect(\text{construction time})&=&{\displaystyle\expect\left(\text{construction time of }T+\sum_i\text{construction time of }T_i\right)}\\
&=&\expect\text{construction time of }T)+{\displaystyle\expect\left(\sum_i\text{construction time of }T_i\right)}\\
&=&O(n)+{\displaystyle\sum_iO(n_i^2)}\\
&=&O(n)+{\displaystyle O\left(\sum_in_i^2\right)}\text{ see Proof 1.4}\\
&=&O(n)
\end{array}\]

\propositionn{FKS Hashing Scheme - Properties}
\begin{itemize}
	\item[-] Has no collisions.
	\item[-] \lstinline!lookup! takes $O(1)$ time in worst-case.
	\item[-] Uses $O(n)$ space.
	\item[-] Can be build in $O(n)$ expected time.
\end{itemize}

\subsection{Cuckoo Hashing}

%TODO something about bucketing
\remark{}
If our consruction has the property that $\forall\ x,y\in S$ with $x\neq y$ the probabiltiy that $x$ and $y$ are in the same bucket is $O\left(\frac1m\right)$, then for any $n$ operations the expected run-time is $O(1)$ per operation.\\

\definition{Cuckoo Hashing}
In \textit{Cuckoo Hasing} we use two hash functions, $h_1\ \&\ h_2$, to produce a single hash table.\\
When we \lstinline!add! a value $x$ to the hash table we place it in position $h_1(x)$. If there is already a value, $y$, already in this position then we move that value, $y$, to its alternative positione. We keep moving values until each value is in its position. If it is not possible (\ie we have found a cycle) then we change $h_1\ \&\ h_2$ for new hash functions and rehash all the values.\\
This is formally descibed in the algorithm below\\
\begin{algorithm}[H]
\SetKwInOut{Require}{require}
\caption{Cuckoo Hashing - Insert}
\Require{$\{x_1,\dots,x_n\}$ $\{\text{stream of keys}\}$, $T\ \{\text{Table with $m$ entries}\}$}
\textbf{choose} $h_1,h_2$
\For{$i\in[1,n]$}{
	$pos=h_1(x)$.\\
	$checked=[]$.\\
	\While{$T[pos]$ not empty}{
		\lIf{$x\in checked$}{\textbf{rehash}}
		$checked$ append $x$.
		$y=T[pos]$.\\
		$T[pos]=x$.\\
		$pos=$alternative position for $y$.\\
		$x=y$.
	}
	$T[pos]=x$.
}
\Return{T}
\end{algorithm}
\nb \textit{Rehash} involves chooseing two new hash functions $h_1\ \&\ h_2$ are reinserting all keys, $\{x_1,\dots,x_n\}$ into the table.\\

\propositionn{Cuckoo Hashing Scheme - Properties}
\begin{enumerate}
	\item An \lstinline!add! takes \textit{amortised exepcted} time $O(1)$.
	\item Every \lstinline!lookup! and every \lstinline!delete! has time complexity $O(1)$ in the worst-case.
	\item The space requirement is $O(n)$ where $n$ is the number of keys stored.
\end{enumerate}

\remark{Assumptions in Cuckoo Hashing}
In \textit{Cuckoo Hashing} we make the following assumptions
\begin{enumerate}
	\item $h_1$ and $h_2$ are indepenent.\\
	\ie $h_1(x)$ says nothing about $h_2(x)$, and visa-versa.
	\item $h_1$ and $h_2$ are truly random.\\
	\ie They map to each entry in the hash table with uniform probability.
	\item Computing the value of $h_1(x)$ and $h_2(x)$ takes $O(1)$ time in the worst-case.
\end{enumerate}

\definition{Cuckoo Graph}
A \textit{Cuckoo Graph} is an interprettation of a \textit{Hash Table} using \textit{Graph Theory}.\\
Each vertex of the graph is an entry in the hash table and for each $x_i$ we add an undirected-edge between $h_1(x_i)$ and $h_2(x_i)$.\\
If any cycles occur in a \textit{Cucko Graph} then we know construction will fail for that pair of hash functions as no stable scenario can occur.\\
The length of the longest path tells us the time for the longest insert.\\

\theorem{Probability of Long Paths in Cuckoo Graphs}
Let $m$ be the size of a hash table \& $n$ the number of entries we wish to insert
Fr any pair of positions $i$ and $j$, and any constant $c>1$, if $m\geq2cn$ then the probability that there exists a shortest path in the cuckoo graph from $i$ to $j$ with length $l\geq1$ is at most $\frac1{c^lm}$.\\

\proof{Theorem 1.5}
TODO\\

\proof{Probability of a path between two positions in a Cuckoo Graph}
If a path exists from $i$ to $j$, ther emust be a shortest path from $i$ to $j$.\\
Therefore we can use \textbf{Theorem 1.5} and the \textit{Union Bound} over all possible paths to show the probability of a path from $i$ to $j$ existing is at most
$$\sum_{l=1}^\infty\frac1{c^lm}=\frac1m\sum_{l=1}^\infty\frac1{c^l}=\frac1m\frac1{c-1}=O\left(\frac1m\right)$$

\definition{Buckets}
We say that two keys $x\ \&\ y$ are in the same \textit{bucket} iff there exists a path from $h_1(x)$ to $h_1(y)$ in a \textit{Cuckoo Graph}.\\
Note that this implies there is a path from $h_1(x),h_2(y)$; $h_2(x),h_1(y)$ and $h_2(x),h_2(y)$ as there are edges $(h_1(x),h_2(x))$ \& $(h_1(y),h_2(y))$.\\

\remark{The time for an operation on $x$ is bounded by the number of items in its bucket.}

\proposition{Probabiltiy of being in the same Bucket}
For $x,y\in S$ with $x\neq y$ the probability that they are in the same bucket is at most
$$\sum_{l=1}^\infty\frac4{c^lm}=\frac4m\sum_{l=1}^\infty\frac1{c^l}=\frac4m\frac1{c-1}=O\left(\frac4m\right)$$
If the size of a hash table is $m\geq2cn$ then the expeceted time per operation is $O(1)$.\\
Further, \lstinline!lookup!s take $O(1)$ time in the worst case.\\

\proposition{Probability of Rehashing}
The probabiltiy that a rehashing occurs in \textit{Cuckoo Hashing} is equal to the probability of the \textit{Cuckoo Graph} having a cycle.\\
A cycle is a path from $x$ to $x$, via some intermidiary vertices.\\
Thus the probability that $x$ is involved in a cycle is
$$\sum_{l=1}^\infty\frac1{c^lm}=\frac1{m(c-1)}$$
by \textbf{Proof 1.7}.\\
Thus the probabiltiy that there is at least one cycle in the whole hash table is
$$m\frac1{m(c-1)}=\frac1{c-1}$$

\proposition{Construction Time - Cuckoo Hashing}
Consider the result in \textbf{Proposition 1.9} when $c=3$.\\
The probability of a rehashing occuring is $\frac12$.\\
Thus we expected only one rehash to be necessary. The the expected time for a rehash is $O(n)$ then the expeceted construction time for the table is $O(n)$.\\
Therefore the \textit{amortised expeced} time for rehashes over $n$ insertions is $O(1)$ per insertion.\\
\nb Checking for a cycle in a graph takes $O(n)$ time.

\newpage
\setcounter{section}{-1}
\section{Reference}

\subsection{Definitions}

\definition{Amortised Expected}
\textit{Amortised Expected} is a term for the complexity of something. It describes the total complexity to execute a sequence of instructions, divided by the number of instructions.\\
\eg If $n$ instructions take $O(n)$ time to execute completley, then the \textit{amortised expected} time is $O(1)$.

\subsection{Probability}

\definition{Sample Space, $\Omega$}
A \textit{Sample Space} is the set of possible outcomes of a scenario. A \textit{Sample Space} is not necessarily finite.\\
\eg Rolling a dice $\Omega:=\{1,2,3,4,5,6\}$.\\

\definition{Event}
An \textit{Event} is a subset of the \textit{Sample Space}.\\
The probability of an \textit{Event}, $A$, happening is
$$\prob(A)=\sum_{x\in A}\prob(x)$$

\definition{Disjoint Events}
Let $A_1\ \&\ A_2$ be events.\\
$A_1\ \&\ A_2$ are said to be \textit{Disjoint} if $A_1\cap A_2=\emptyset$.\\

\definition{$\sigma$-Field, $\mathcal{F}$}
A \textit{Sigma Field} is the set of possible events in a given scenario.\\
A \textit{Sigma Field} must fulfil the following criteria
\begin{enumerate}
	\item $\emptyset,\Omega\in\mathcal{F}$.
	\item $\forall\ A\in\mathcal{F}\implies A^c\in\mathcal{F}$.
	\item $\forall\ \{A_1,\dots,A_n\}\subseteq\mathcal{F}\implies\bigcup\limits_{i=1}^nA_i\in\mathcal{F}$.
\end{enumerate}

\definition{Probability Measure, $\prob$}
A \textit{Probability Measure} maps a \textit{$\sigma$-Field} to $[0,1]$ which satisfies
\begin{enumerate}
	\item $\prob(\emptyset)=0$ \&\ $\prob(S)=1$; and,
	\item If $\{A_1,\dots,A_n\}\subseteq\mathcal{F}$ are pair-wise disjoint then $\prob\left(\bigcup\limits_{i=1}^nA_i\right)=\sum_{i=1}^n\prob(A_i)$. [$\sigma$-Additivity]
\end{enumerate}

\definition{Random Variable}
A \textit{Random Variable} is a function from the sample space, $S$, to the real numbers, $\reals$.\\
$$X:S\to\reals$$
The probability of a \textit{Random Variable}, $X$, taking a specific value $x$ is found by
$$\prob(X=x)=\sum_{\{a\in \Omega:X(a)=x\}}\prob(a)$$

\definition{Indicator Random Variable}
An \textit{Indicator Random Variable} is a \textit{Random Variable} which only ever takes $0$ or $1$ and is used to indicate whether a particular event has happened (1), or not (0).
$$\expect(I)=\prob(I=1)$$

\definition{Expected Value, $\expect$}
The \textit{Expected Value} of a \textit{Random Variable} is the mean value of said \textit{Random Variable}
$$\expect(X):=\sum_x x\prob(X=x)$$

\theorem{Linearity of Expected Value}
Let $X_1,\dots,X_n$ be random variables. Then
$$\expect\left(\sum_{i=1}^nX_i\right)=\sum_{i=1}^n\expect(X_i)$$

\theorem{Markov's Inequality}
Let $X$ be a non-negative random variable. Then
$$\prob(X\geq a)\leq\frac1a\expect(X)\quad\forall\ a>0$$

\theorem{Union Bound}
Let $A_1,\dots,A_n$ be \textit{Events}. Then
$$\prob\left(\bigcup\limits_{i=1}^nA_i)\right)\leq\sum_{i=1}^n\prob(A_i)$$
\nb This in an equality if the events are disjoint.\\

\proof{Union Bound}
Define \textit{Indicator RV} $I_i$ st
$$I_i:=\begin{cases}1&A_i\text{ happened}\\0&\text{otherwise}\end{cases}$$
Define \textit{Random Variable} $X:=\sum_{i=1}^nI_i$ (the number of events that happened).\\
Then
\[\begin{array}{rcl}
\prob\left(\bigcup\limits_{i=1}^nA_i)\right)&=&\prob(X>0)\\
&\leq&\expect(X)\text{ by Markov's Inequality}\\
&=&\expect\left[\displaystyle\sum_{i=1}^nI_i\right]\\
&=&\displaystyle\sum_{i=1}^n\expect[I_i]\\
&=&\displaystyle\sum_{i=1}^n\prob(I_i=1)\\
&=&\displaystyle\sum_{i=1}^n\prob(A_i1)
\end{array}\]
\proved

\end{document}
